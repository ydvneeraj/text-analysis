{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.chunk.regexp import ChunkString, ChunkRule \n",
    "from nltk.tree import Tree \n",
    "  \n",
    "# ChunkString() starts with the flat tree \n",
    "tree = Tree('S', [('the', 'DT'), ('book', 'NN'), \n",
    "               ('has', 'VBZ'), ('many', 'JJ'), ('chapters', 'NNS')]) \n",
    "tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk String :   <DT>  <NN>  <VBZ>  <JJ>  <NNS> \n"
     ]
    }
   ],
   "source": [
    "chunk_string = ChunkString(tree) \n",
    "print (\"Chunk String : \", chunk_string) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "  NP: {<DT|JJ|NN.*>+}          # Chunk sequences of DT, JJ, NN\n",
    "  PP: {<IN><NP>}               # Chunk prepositions followed by NP\n",
    "  VP: {<VB.*>} # Chunk verbs and their arguments\n",
    "        \n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = nltk.RegexpParser(grammar) \n",
    "sent=chunker.parse(tree)\n",
    "sent.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'mother', 'is', 'cooking', 'food', 'for', 'all', 'the', 'family', 'member']\n",
      "[('My', 'PRP$'), ('mother', 'NN'), ('is', 'VBZ'), ('cooking', 'VBG'), ('food', 'NN'), ('for', 'IN'), ('all', 'PDT'), ('the', 'DT'), ('family', 'NN'), ('member', 'NN')]\n",
      "(S\n",
      "  My/PRP$\n",
      "  (NP mother/NN)\n",
      "  is/VBZ\n",
      "  cooking/VBG\n",
      "  (NP food/NN)\n",
      "  for/IN\n",
      "  all/PDT\n",
      "  (NP the/DT family/NN)\n",
      "  (NP member/NN))\n"
     ]
    }
   ],
   "source": [
    "text = \"My mother is cooking food for all the family member\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)\n",
    "tag = nltk.pos_tag(tokens)\n",
    "print(tag)\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp  =nltk.RegexpParser(grammar)\n",
    "result = cp.parse(tag)\n",
    "print(result)\n",
    "result.draw()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT sushi/NN roll/NN)\n",
      "  was/VBD\n",
      "  filled/VBN\n",
      "  (NP with/IN the/DT fish/NN))\n"
     ]
    }
   ],
   "source": [
    "chunker = RegexpParser(r''' \n",
    "NP: \n",
    "{<DT><NN.*><.*>*<NN.*>} \n",
    "}<VB.*>{ \n",
    "''') \n",
    "sent = [('the', 'DT'), ('sushi', 'NN'), ('roll', 'NN'), ('was', 'VBD'),  \n",
    "        ('filled', 'VBN'), ('with', 'IN'), ('the', 'DT'), ('fish', 'NN')] \n",
    "result2=chunker.parse(sent) \n",
    "print(result2)\n",
    "result2.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('each', 'DT'), ('one', 'CD'), ('plant', 'NN'), ('one', 'CD')]\n",
      "[('Plants', 'NNS'), ('required', 'VBN'), ('light', 'JJ'), ('and', 'CC'), ('water', 'NN'), ('to', 'TO'), ('grow', 'VB')]\n"
     ]
    }
   ],
   "source": [
    "sentence=nltk.word_tokenize(\"each one plant one\")\n",
    "sentence1=nltk.word_tokenize(\"Plants required light and water to grow\")\n",
    "sentence=nltk.pos_tag(sentence)\n",
    "sentence1=nltk.pos_tag(sentence1)\n",
    "print(sentence)\n",
    "print(sentence1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('VBG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('plant.v.06')\n",
      "Synset('plant.n.04')\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "print(lesk(sentence, 'plant'))\n",
    "print(lesk(sentence1, 'plant','n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('plant.n.01') buildings for carrying on industrial labor\n",
      "Synset('plant.n.02') (botany) a living organism lacking the power of locomotion\n",
      "Synset('plant.n.03') an actor situated in the audience whose acting is rehearsed but seems spontaneous to the audience\n",
      "Synset('plant.n.04') something planted secretly for discovery by another\n",
      "Synset('plant.v.01') put or set (seeds, seedlings, or plants) into the ground\n",
      "Synset('implant.v.01') fix or set securely or deeply\n",
      "Synset('establish.v.02') set up or lay the groundwork for\n",
      "Synset('plant.v.04') place into a river\n",
      "Synset('plant.v.05') place something or someone in a certain position in order to secretly observe or deceive\n",
      "Synset('plant.v.06') put firmly in the mind\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "for ss in wn.synsets('plant'):\n",
    "    print(ss,ss.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NE WASHINGTON/NNP)\n",
      "  --/:\n",
      "  In/IN\n",
      "  the/DT\n",
      "  wake/NN\n",
      "  of/IN\n",
      "  a/DT\n",
      "  string/NN\n",
      "  of/IN\n",
      "  abuses/NNS\n",
      "  by/IN\n",
      "  (NE New/NNP York/NNP)\n",
      "  police/NN\n",
      "  officers/NNS\n",
      "  in/IN\n",
      "  the/DT\n",
      "  1990s/CD\n",
      "  ,/,\n",
      "  (NE Loretta/NNP)\n",
      "  E./NNP\n",
      "  Lynch/NNP\n",
      "  ,/,\n",
      "  the/DT\n",
      "  top/JJ\n",
      "  federal/JJ\n",
      "  prosecutor/NN\n",
      "  in/IN\n",
      "  (NE Brooklyn/NNP)\n",
      "  ,/,\n",
      "  spoke/VBD\n",
      "  forcefully/RB\n",
      "  about/IN\n",
      "  the/DT\n",
      "  pain/NN\n",
      "  of/IN\n",
      "  a/DT\n",
      "  broken/JJ\n",
      "  trust/NN\n",
      "  that/IN\n",
      "  African-Americans/NNP\n",
      "  felt/VBD\n",
      "  and/CC\n",
      "  said/VBD\n",
      "  the/DT\n",
      "  responsibility/NN\n",
      "  for/IN\n",
      "  repairing/VBG\n",
      "  generations/NNS\n",
      "  of/IN\n",
      "  miscommunication/NN\n",
      "  and/CC\n",
      "  mistrust/NN\n",
      "  fell/VBD\n",
      "  to/TO\n",
      "  law/NN\n",
      "  enforcement/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "my_sent = \"WASHINGTON -- In the wake of a string of abuses by New York police officers in the 1990s, Loretta E. Lynch, the top federal prosecutor in Brooklyn, spoke forcefully about the pain of a broken trust that African-Americans felt and said the responsibility for repairing generations of miscommunication and mistrust fell to law enforcement.\"\n",
    "\n",
    "parse_tree = nltk.ne_chunk(nltk.tag.pos_tag(nltk.word_tokenize(my_sent)), binary=True)  # POS tagging before chunking!\n",
    "\n",
    "parse_tree.draw()\n",
    "print(parse_tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('NE', [('WASHINGTON', 'NNP')]), Tree('NE', [('New', 'NNP'), ('York', 'NNP')]), Tree('NE', [('Loretta', 'NNP')]), Tree('NE', [('Brooklyn', 'NNP')])]\n"
     ]
    }
   ],
   "source": [
    "named_entities = []\n",
    "\n",
    "for t in parse_tree.subtrees():\n",
    "    if t.label() == 'NE':\n",
    "        named_entities.append(t)\n",
    "        # named_entities.append(list(t))  # if you want to save a list of tagged words instead of a tree\n",
    "\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('GPE', 'WASHINGTON')\n",
      "('GPE', 'New York')\n",
      "('PERSON', 'Loretta E.')\n"
     ]
    }
   ],
   "source": [
    "from nltk.tree import Tree\n",
    "\n",
    "txt=\"WASHINGTON -- In the wake of a string of abuses by New York police officers in the 1990s, Loretta E. Lynch, the top federal prosecutor in Brooklyn, spoke forcefully about the pain of a broken trust that African-Americans felt and said the responsibility for repairing generations of miscommunication and mistrust fell to law enforcement.\"\n",
    "\n",
    "pos_tag = nltk.pos_tag(txt.split())\n",
    "parse_tree = nltk.ne_chunk(pos_tag )\n",
    "# print(chunk)\n",
    "parse_tree.draw()\n",
    "NE=[]\n",
    "for chunk in parse_tree:\n",
    "    if hasattr(chunk, 'label'):\n",
    "        NE=(chunk.label(), ' '.join(c[0] for c in chunk))\n",
    "        print(NE)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WASHINGTON', 'New York', 'Loretta E. Lynch', 'Brooklyn']\n"
     ]
    }
   ],
   "source": [
    "word = nltk.word_tokenize(my_sent)   \n",
    "pos_tag = nltk.pos_tag(word)   \n",
    "chunk = nltk.ne_chunk(pos_tag)   \n",
    "NE = [ \" \".join(w for w, t in name) for name in chunk if isinstance(name, nltk.Tree)]   \n",
    "print (NE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WASHINGTON\n",
      "New York\n",
      "Loretta E. Lynch\n",
      "Brooklyn\n"
     ]
    }
   ],
   "source": [
    "word = nltk.word_tokenize(my_sent)   \n",
    "pos_tag = nltk.pos_tag(word)   \n",
    "chunk = nltk.ne_chunk(pos_tag)   \n",
    "for name in chunk:\n",
    "    if isinstance(name,nltk.Tree):\n",
    "        ne=\" \".join(w for w, t in name)\n",
    "        print(ne)\n",
    "       \n",
    "\n",
    "# NE = [ \" \".join(w for w, t in name) for name in chunk if isinstance(name, nltk.Tree)]   \n",
    "# print (NE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Split: ['Tom', 'should', 'have', 'gone', 'to', 'the', 'dentist', 'yesterday.']\n",
      "After Token: [('Tom', 'NNP'), ('should', 'MD'), ('have', 'VB'), ('gone', 'VBN'), ('to', 'TO'), ('the', 'DT'), ('dentist', 'NN'), ('yesterday.', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk Tom/NNP)\n",
      "  should/MD\n",
      "  have/VB\n",
      "  gone/VBN\n",
      "  to/TO\n",
      "  the/DT\n",
      "  (mychunk dentist/NN yesterday./NN))\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import RegexpParser\n",
    "text =\"Tom should have gone to the dentist yesterday.\".split()\n",
    "print(\"After Split:\",text)\n",
    "tokens_tag = pos_tag(text)\n",
    "print(\"After Token:\",tokens_tag)\n",
    "patterns= \"\"\"mychunk:{<NN.?>*<VBD.?>*<JJ.?>*<CC>?}\"\"\"\n",
    "chunker = RegexpParser(patterns)\n",
    "print(\"After Regex:\",chunker)\n",
    "output = chunker.parse(tokens_tag)\n",
    "print(\"After Chunking\",output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"my mother is cooking food\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S my/PRP$ mother/NN is/VBZ cooking/VBG food/NN)\n"
     ]
    }
   ],
   "source": [
    "chunk=nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sentence)))\n",
    "print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
